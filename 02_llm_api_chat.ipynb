{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Basic LLM Chat Tool\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this part, you'll create a simple command-line chat tool that interacts with a Large Language Model (LLM) through the Hugging Face API. This tool will allow you to have conversations with an LLM about healthcare topics.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Connect to the Hugging Face API\n",
    "- Create a basic interactive chat loop\n",
    "- Handle simple error cases\n",
    "- Test with healthcare questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "from typing import Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('utils', exist_ok=True)\n",
    "os.makedirs('results/part_2', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connecting to the Hugging Face API\n",
    "\n",
    "The Hugging Face Inference API provides access to many language models. We'll use models that are available on the free tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a simple API request to Hugging Face\n",
    "# API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-base\" # cannot find this model\n",
    "API_URL = \"https://router.huggingface.co/cohere/compatibility/v1/chat/completions\"\n",
    "from keys import HF_API_KEY  # Import your Hugging Face API key\n",
    "headers = {\"Authorization\": f\"Bearer {HF_API_KEY}\"}  # Optional for some models\n",
    "\n",
    "def query(payload):\n",
    "    \"\"\"\n",
    "    Send a query to the Hugging Face API\n",
    "    \n",
    "    Args:\n",
    "        payload: Dictionary containing the query parameters\n",
    "        \n",
    "    Returns:\n",
    "        The API response\n",
    "    \"\"\"\n",
    "    # TODO: Implement the API request\n",
    "    # Use requests.post to send the query to the API_URL\n",
    "    # Return the response\n",
    "\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        logger.error(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diabetes is a chronic condition that affects the way the body regulates blood sugar, or glucose. There are two main types of diabetes: type 1 and type 2. The symptoms of diabetes can vary depending on the type and the individual, but some common symptoms include:\n",
      "\n",
      "- Frequent urination: People with diabetes may urinate more often than usual, as the body tries to get rid of the excess glucose in the blood.\n",
      "- Increased thirst: As the body tries to replace the fluid lost through frequent urination, people with diabetes may feel more thirsty than usual.\n",
      "- Hunger: The body may not be able to get enough energy from the food eaten, leading to increased hunger.\n",
      "- Weight loss: Despite eating more, people with diabetes may lose weight as the body breaks down muscle and fat for energy.\n",
      "- Fatigue: The body may feel tired and weak as it is not getting enough energy from food.\n",
      "- Blurred vision: High blood sugar levels can cause the lenses of the eyes to swell, leading to blurred vision.\n",
      "- Slow-healing sores or frequent infections: Diabetes can affect the body's ability to heal and fight off infections.\n",
      "- Numbness or tingling in the hands or feet: High blood sugar levels can damage the nerves, leading to numbness or tingling in the extremities.\n",
      "\n",
      "It is important to note that not everyone with diabetes will experience all of these symptoms, and some people may not experience any symptoms at all. If you are concerned that you may have diabetes, it is important to speak to a doctor or healthcare professional for a proper diagnosis and treatment plan.\n"
     ]
    }
   ],
   "source": [
    "# Test the query function\n",
    "test_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What are the symptoms of diabetes?\"\n",
    "        }\n",
    "    ],\n",
    "    \"model\": \"command-r-plus-04-2024\"\n",
    "}\n",
    "# {\"inputs\": \"What are the symptoms of diabetes?\"}\n",
    "response = query(test_payload)\n",
    "print(response['choices'][0]['message']['content'] if response else \"No response received.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Simple Chat Scripts\n",
    "\n",
    "Your task is to create two simple scripts that interact with the Hugging Face API:\n",
    "\n",
    "1. A basic one-off chat script (`utils/one_off_chat.py`)\n",
    "2. A contextual conversation script (`utils/conversation.py`)\n",
    "\n",
    "### One-Off Chat Script\n",
    "\n",
    "Create a script that handles independent interactions (each prompt/response is separate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils/one_off_chat.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/one_off_chat.py\n",
    "# utils/one_off_chat.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model MODEL] [--prompt PROMPT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/Users/macbook/Library/Jupyter/runtime/kernel-v3373b6908a90486a9d10f5e5c57d1ac89c7666edf.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/Documents/ucsf_couses/ds223/assignments/7-transformers-Exynos-8890/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3678: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "def get_response(prompt, model_name=\"command-r-plus-04-2024\", api_key=None):\n",
    "    \"\"\"\n",
    "    Get a response from the model\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the model\n",
    "        model_name: Name of the model to use\n",
    "        api_key: API key for authentication (optional for some models)\n",
    "        \n",
    "    Returns:\n",
    "        The model's response\n",
    "    \"\"\"\n",
    "    # TODO: Implement the get_response function\n",
    "    # Set up the API URL and headers\n",
    "    # Create a payload with the prompt\n",
    "    # Send the payload to the API\n",
    "    # Extract and return the generated text from the response\n",
    "    # Handle any errors that might occur\n",
    "\n",
    "    payload = {\n",
    "        'messages': [\n",
    "            {'role': 'user', 'content': prompt}\n",
    "        ],\n",
    "        'model': model_name\n",
    "    }\n",
    "    response = query(payload)\n",
    "    if response and 'choices' in response:\n",
    "        return response['choices'][0]['message']['content']\n",
    "    else:\n",
    "        logger.error(\"Failed to get a valid response from the model.\")\n",
    "        return None\n",
    "    pass\n",
    "\n",
    "def run_chat():\n",
    "    \"\"\"Run an interactive chat session\"\"\"\n",
    "    print(\"Welcome to the Simple LLM Chat! Type 'exit' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        # TODO: Get response from the model\n",
    "        # Print the response\n",
    "        response = get_response(user_input)\n",
    "        if response:\n",
    "            print(f\"Model: {response}\")\n",
    "        else:\n",
    "            print(\"Model: Sorry, I couldn't process that.\")\n",
    "        \n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Chat with an LLM\")\n",
    "    # TODO: Add arguments to the parser\n",
    "    parser.add_argument('--model', type=str, default='command-r-plus-04-2024', help='Model name to use for chat')\n",
    "    parser.add_argument('--prompt', type=str, help='Initial prompt to send to the model')\n",
    "    args = parser.parse_args()\n",
    "    model = args.model\n",
    "    prompt = args.prompt\n",
    "    response = get_response(prompt, model_name=model) if prompt else None\n",
    "    if response:\n",
    "        print(f\"Model: {response}\")\n",
    "    else:\n",
    "        print(\"Model: Sorry, I couldn't process that.\")\n",
    "    \n",
    "    \n",
    "    # TODO: Run the chat function with parsed arguments\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Conversation Script\n",
    "\n",
    "Create a script that maintains conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils/conversation.py\n",
    "# utils/conversation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt, history=None, model_name=\"google/flan-t5-base\", api_key=None, history_length=3):\n",
    "    \"\"\"\n",
    "    Get a response from the model using conversation history\n",
    "    \n",
    "    Args:\n",
    "        prompt: The current user prompt\n",
    "        history: List of previous (prompt, response) tuples\n",
    "        model_name: Name of the model to use\n",
    "        api_key: API key for authentication\n",
    "        history_length: Number of previous exchanges to include in context\n",
    "        \n",
    "    Returns:\n",
    "        The model's response\n",
    "    \"\"\"\n",
    "    # TODO: Implement the contextual response function\n",
    "    # Initialize history if None\n",
    "    if history is None:\n",
    "        history = []\n",
    "        \n",
    "    # TODO: Format a prompt that includes previous exchanges\n",
    "    # Get a response from the API\n",
    "    # Return the response\n",
    "    pass\n",
    "\n",
    "def run_chat():\n",
    "    \"\"\"Run an interactive chat session with context\"\"\"\n",
    "    print(\"Welcome to the Contextual LLM Chat! Type 'exit' to quit.\")\n",
    "    \n",
    "    # Initialize conversation history\n",
    "    history = []\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        # TODO: Get response using conversation history\n",
    "        # Update history\n",
    "        # Print the response\n",
    "        \n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Chat with an LLM using conversation history\")\n",
    "    # TODO: Add arguments to the parser\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # TODO: Run the chat function with parsed arguments\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils/conversation.py\n",
    "# utils/conversation.py\n",
    "\n",
    "import requests\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def get_response(prompt, history=None, model_name=\"google/flan-t5-base\", api_key=None, history_length=3):\n",
    "    \"\"\"\n",
    "    Get a response from the model using conversation history\n",
    "    \n",
    "    Args:\n",
    "        prompt: The current user prompt\n",
    "        history: List of previous (prompt, response) tuples\n",
    "        model_name: Name of the model to use\n",
    "        api_key: API key for authentication\n",
    "        history_length: Number of previous exchanges to include in context\n",
    "        \n",
    "    Returns:\n",
    "        The model's response\n",
    "    \"\"\"\n",
    "    # TODO: Implement the contextual response function\n",
    "    # Initialize history if None\n",
    "    if history is None:\n",
    "        history = []\n",
    "        \n",
    "    # TODO: Format a prompt that includes previous exchanges\n",
    "    # Get a response from the API\n",
    "    # Return the response\n",
    "    pass\n",
    "\n",
    "def run_chat():\n",
    "    \"\"\"Run an interactive chat session with context\"\"\"\n",
    "    print(\"Welcome to the Contextual LLM Chat! Type 'exit' to quit.\")\n",
    "    \n",
    "    # Initialize conversation history\n",
    "    history = []\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        # TODO: Get response using conversation history\n",
    "        # Update history\n",
    "        # Print the response\n",
    "        \n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Chat with an LLM using conversation history\")\n",
    "    # TODO: Add arguments to the parser\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # TODO: Run the chat function with parsed arguments\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils/conversation.py\n",
    "# utils/conversation.py\n",
    "\n",
    "import requests\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def get_response(prompt, history=None, model_name=\"google/flan-t5-base\", api_key=None, history_length=3):\n",
    "    \"\"\"\n",
    "    Get a response from the model using conversation history\n",
    "    \n",
    "    Args:\n",
    "        prompt: The current user prompt\n",
    "        history: List of previous (prompt, response) tuples\n",
    "        model_name: Name of the model to use\n",
    "        api_key: API key for authentication\n",
    "        history_length: Number of previous exchanges to include in context\n",
    "        \n",
    "    Returns:\n",
    "        The model's response\n",
    "    \"\"\"\n",
    "    # TODO: Implement the contextual response function\n",
    "    # Initialize history if None\n",
    "    if history is None:\n",
    "        history = []\n",
    "        \n",
    "    # TODO: Format a prompt that includes previous exchanges\n",
    "    # Get a response from the API\n",
    "    # Return the response\n",
    "    pass\n",
    "\n",
    "def run_chat():\n",
    "    \"\"\"Run an interactive chat session with context\"\"\"\n",
    "    print(\"Welcome to the Contextual LLM Chat! Type 'exit' to quit.\")\n",
    "    \n",
    "    # Initialize conversation history\n",
    "    history = []\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        # TODO: Get response using conversation history\n",
    "        # Update history\n",
    "        # Print the response\n",
    "        \n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Chat with an LLM using conversation history\")\n",
    "    # TODO: Add arguments to the parser\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # TODO: Run the chat function with parsed arguments\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing and Evaluation\n",
    "\n",
    "Create a script to test your chat implementations with specific healthcare questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils/test_chat.py\n",
    "# utils/test_chat.py\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our chat modules - since we're in the same directory\n",
    "from one_off_chat import get_response as get_one_off_response\n",
    "# Optionally import the conversation module if testing that too\n",
    "# from conversation import get_response as get_contextual_response\n",
    "\n",
    "def test_chat(questions, model_name=\"google/flan-t5-base\", api_key=None):\n",
    "    \"\"\"\n",
    "    Test the chat function with a list of questions\n",
    "    \n",
    "    Args:\n",
    "        questions: A list of questions to test\n",
    "        model_name: Name of the model to use\n",
    "        api_key: API key for authentication\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary mapping questions to responses\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"Testing question: {question}\")\n",
    "        # Get response using the one-off chat function\n",
    "        response = get_one_off_response(question, model_name, api_key)\n",
    "        results[question] = response\n",
    "        \n",
    "    return results\n",
    "\n",
    "def save_results(results, output_file=\"results/part_2/example.txt\"):\n",
    "    \"\"\"\n",
    "    Save the test results to a file\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary mapping questions to responses\n",
    "        output_file: Path to the output file\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        # Write header\n",
    "        f.write(\"# LLM Chat Tool Test Results\\n\\n\")\n",
    "        \n",
    "        # Write usage examples\n",
    "        f.write(\"## Usage Examples\\n\\n\")\n",
    "        f.write(\"```bash\\n\")\n",
    "        f.write(\"# Run the one-off chat\\n\")\n",
    "        f.write(\"python utils/one_off_chat.py\\n\\n\")\n",
    "        f.write(\"# Run the contextual chat\\n\")\n",
    "        f.write(\"python utils/conversation.py\\n\")\n",
    "        f.write(\"```\\n\\n\")\n",
    "        \n",
    "        # Write test results\n",
    "        f.write(\"## Test Results\\n\\n\")\n",
    "        f.write(\"```csv\\n\")\n",
    "        f.write(\"question,response\\n\")\n",
    "        \n",
    "        for question, response in results.items():\n",
    "            # Format the question and response for CSV\n",
    "            q = question.replace(',', '').replace('\\n', ' ')\n",
    "            r = response.replace(',', '').replace('\\n', ' ')\n",
    "            f.write(f\"{q},{r}\\n\")\n",
    "            \n",
    "        f.write(\"```\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of healthcare questions to test\n",
    "    test_questions = [\n",
    "        \"What are the symptoms of gout?\",\n",
    "        \"How is gout diagnosed?\",\n",
    "        \"What treatments are available for gout?\",\n",
    "        \"What lifestyle changes can help manage gout?\",\n",
    "        \"What foods should be avoided with gout?\"\n",
    "    ]\n",
    "    \n",
    "    results = test_chat(test_questions)\n",
    "    save_results(results)\n",
    "    print(\"Test results saved to results/part_2/example.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of healthcare questions to test\n",
    "test_questions = [\n",
    "    \"What are the symptoms of gout?\",\n",
    "    \"How is gout diagnosed?\",\n",
    "    \"What treatments are available for gout?\",\n",
    "    \"What lifestyle changes can help manage gout?\",\n",
    "    \"What foods should be avoided with gout?\"\n",
    "]\n",
    "\n",
    "# Display the test questions\n",
    "print(\"Test questions:\")\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"{i}. {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Checkpoints\n",
    "\n",
    "1. **API Connection**:\n",
    "   - [ ] Successfully connect to the Hugging Face API\n",
    "   - [ ] Send a query and receive a response\n",
    "   - [ ] Handle API errors gracefully\n",
    "\n",
    "2. **Chat Function Implementation**:\n",
    "   - [ ] Implement the get_response function\n",
    "   - [ ] Create the run_chat function for interactive sessions\n",
    "   - [ ] Handle errors and edge cases\n",
    "\n",
    "3. **Command Line Interface**:\n",
    "   - [ ] Create a parser with appropriate arguments\n",
    "   - [ ] Implement the main function\n",
    "   - [ ] Test the CLI functionality\n",
    "\n",
    "4. **Testing and Evaluation**:\n",
    "   - [ ] Test the functions with healthcare questions\n",
    "   - [ ] Save the results in a structured format\n",
    "   - [ ] Analyze the quality of responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Issues and Solutions\n",
    "\n",
    "1. **API Access Issues**:\n",
    "   - Problem: Rate limiting\n",
    "   - Solution: Implement exponential backoff and retry logic\n",
    "   - Problem: Authentication errors\n",
    "   - Solution: Verify API key and environment variables\n",
    "\n",
    "2. **Response Parsing Issues**:\n",
    "   - Problem: Unexpected response format\n",
    "   - Solution: Add error handling for different response structures\n",
    "   - Problem: Empty or error responses\n",
    "   - Solution: Provide meaningful fallback responses\n",
    "\n",
    "3. **CLI Issues**:\n",
    "   - Problem: Arguments not parsed correctly\n",
    "   - Solution: Test with different argument combinations\n",
    "   - Problem: Script not executable\n",
    "   - Solution: Check file permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Submit\n",
    "\n",
    "1. Your implementation of the chat scripts:\n",
    "   - Basic requirement: `utils/one_off_chat.py` for single prompt/response chat\n",
    "   - Stretch goal (optional): `utils/conversation.py` for contextual chat\n",
    "   - Testing script: `utils/test_chat.py` to evaluate your implementation\n",
    "\n",
    "2. Test results in `results/part_2/example.txt` with the following format:\n",
    "   - Usage examples section showing how to run your scripts\n",
    "   - Test results section with CSV-formatted question/response pairs\n",
    "   - If you implemented the stretch goal, include examples of contextual exchanges\n",
    "\n",
    "The auto-grader should check:\n",
    "1. That your chat scripts can be executed\n",
    "2. That they correctly handle the test questions\n",
    "3. That your results file contains the required sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test your implementation\n",
    "# Make sure to complete the TODO items in the scripts first!\n",
    "\n",
    "# Run the test script (uncomment when your implementation is ready)\n",
    "# %run utils/test_chat.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
