{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Basic LLM Chat Tool\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this part, you'll create a simple command-line chat tool that interacts with a Large Language Model (LLM) through the Hugging Face API. This tool will allow you to have conversations with an LLM about healthcare topics.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Connect to the Hugging Face API\n",
    "- Create a basic interactive chat loop\n",
    "- Handle simple error cases\n",
    "- Test with healthcare questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "from typing import Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('utils', exist_ok=True)\n",
    "os.makedirs('results/part_2', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connecting to the Hugging Face API\n",
    "\n",
    "The Hugging Face Inference API provides access to many language models. We'll use models that are available on the free tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a simple API request to Hugging Face\n",
    "# API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-base\" # cannot find this model\n",
    "API_URL = \"https://router.huggingface.co/cohere/compatibility/v1/chat/completions\"\n",
    "from keys import HF_API_KEY  # Import your Hugging Face API key\n",
    "headers = {\"Authorization\": f\"Bearer {HF_API_KEY}\"}  # Optional for some models\n",
    "\n",
    "def query(payload):\n",
    "    \"\"\"\n",
    "    Send a query to the Hugging Face API\n",
    "    \n",
    "    Args:\n",
    "        payload: Dictionary containing the query parameters\n",
    "        \n",
    "    Returns:\n",
    "        The API response\n",
    "    \"\"\"\n",
    "    # TODO: Implement the API request\n",
    "    # Use requests.post to send the query to the API_URL\n",
    "    # Return the response\n",
    "\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        logger.error(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 23:31:00,932 - __main__ - ERROR - Error: 401 - {\"error\":\"Invalid credentials in Authorization header\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No response received.\n"
     ]
    }
   ],
   "source": [
    "# Test the query function\n",
    "test_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What are the symptoms of diabetes?\"\n",
    "        }\n",
    "    ],\n",
    "    \"model\": \"command-r-plus-04-2024\"\n",
    "}\n",
    "# {\"inputs\": \"What are the symptoms of diabetes?\"}\n",
    "response = query(test_payload)\n",
    "print(response['choices'][0]['message']['content'] if response else \"No response received.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Simple Chat Scripts\n",
    "\n",
    "Your task is to create two simple scripts that interact with the Hugging Face API:\n",
    "\n",
    "1. A basic one-off chat script (`utils/one_off_chat.py`)\n",
    "2. A contextual conversation script (`utils/conversation.py`)\n",
    "\n",
    "### One-Off Chat Script\n",
    "\n",
    "Create a script that handles independent interactions (each prompt/response is separate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils/one_off_chat.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/one_off_chat.py\n",
    "# utils/one_off_chat.py\n",
    "import argparse\n",
    "import requests\n",
    "API_URL = \"https://router.huggingface.co/cohere/compatibility/v1/chat/completions\"\n",
    "# from keys import HF_API_KEY  # Import your Hugging Face API key\n",
    "\n",
    "def get_response(prompt:str,api_key, model_name=\"command-r-plus-04-2024\"):\n",
    "    \"\"\"\n",
    "    Get a response from the model\n",
    "\n",
    "    Args:\n",
    "        prompt: The prompt to send to the model\n",
    "        model_name: Name of the model to use\n",
    "        api_key: API key for authentication (optional for some models)\n",
    "\n",
    "    Returns:\n",
    "        The model's response\n",
    "    \"\"\"\n",
    "    # TODO: Implement the get_response function\n",
    "    # Set up the API URL and headers\n",
    "    # Create a payload with the prompt\n",
    "    # Send the payload to the API\n",
    "    # Extract and return the generated text from the response\n",
    "    # Handle any errors that might occur\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}  # Optional for some models\n",
    "\n",
    "    payload = {\n",
    "        'messages': [\n",
    "            {'role': 'user', 'content': prompt}\n",
    "        ],\n",
    "        'model': model_name\n",
    "    }\n",
    "    response= requests.post(API_URL, headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        response = response.json()\n",
    "        return response['choices'][0]['message']['content']\n",
    "    else:\n",
    "        print(\"Failed to get a valid response from the model.\")\n",
    "        return None\n",
    "    pass\n",
    "\n",
    "def run_chat(api_key):\n",
    "    \"\"\"Run an interactive chat session\"\"\"\n",
    "    print(\"Welcome to the Simple LLM Chat! Type 'exit' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # TODO: Get response from the model\n",
    "        response = get_response(user_input, api_key)\n",
    "        if response:\n",
    "            print(f\"Model: {response}\")\n",
    "        else:\n",
    "            print(\"Model: Sorry, I couldn't process that.\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Chat with an LLM\")\n",
    "    # TODO: Add arguments to the parser\n",
    "    parser.add_argument('--prompt', type=str,default= '', help='Initial prompt to send to the model')\n",
    "    parser.add_argument('--key', type=str, required = True ,help='Hugging Face API key (required)')\n",
    "    args = parser.parse_args()\n",
    "    key = args.key\n",
    "    prompt = args.prompt\n",
    "\n",
    "    # TODO: Run the chat function with parsed arguments\n",
    "    if prompt == '':\n",
    "        run_chat(api_key=key)\n",
    "    else:\n",
    "        response = get_response(prompt, key)\n",
    "        if response:\n",
    "            print(f\"Model: {response}\")\n",
    "        else:\n",
    "            print(\"Model: Sorry, I couldn't process that.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Conversation Script\n",
    "\n",
    "Create a script that maintains conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils/conversation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/conversation.py\n",
    "# utils/conversation.py\n",
    "\n",
    "import requests\n",
    "import argparse\n",
    "import os\n",
    "API_URL = \"https://router.huggingface.co/cohere/compatibility/v1/chat/completions\"\n",
    "def get_response(prompt: str,api_key:str, history=None, model_name=\"command-r-plus-04-2024\", history_length=3):\n",
    "    \"\"\"\n",
    "    Get a response from the model using conversation history\n",
    "\n",
    "    Args:\n",
    "        prompt: The current user prompt\n",
    "        history: List of previous (prompt, response) tuples\n",
    "        model_name: Name of the model to use\n",
    "        api_key: API key for authentication\n",
    "        history_length: Number of previous exchanges to include in context\n",
    "\n",
    "    Returns:\n",
    "        The model's response\n",
    "    \"\"\"\n",
    "    # TODO: Implement the contextual response function\n",
    "    # Initialize history if None\n",
    "    if history is None:\n",
    "        history = list()\n",
    "    history.append(\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    )\n",
    "    payload = {\n",
    "        'messages': history[(history_length * -1):],\n",
    "        'model': model_name,\n",
    "    }\n",
    "\n",
    "    # TODO: Format a prompt that includes previous exchanges\n",
    "    # Get a response from the API\n",
    "    # Return the response\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}  # Optional for some models\n",
    "    response= requests.post(API_URL, headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        response = response.json()\n",
    "        # print(response)\n",
    "        return response['choices'][0]['message']['content']\n",
    "    else:\n",
    "        print(\"Failed to get a valid response from the model.\")\n",
    "        return None\n",
    "\n",
    "def run_chat(api_key: str):\n",
    "    \"\"\"Run an interactive chat session with context\"\"\"\n",
    "    print(\"Welcome to the Contextual LLM Chat! Type 'exit' to quit.\")\n",
    "\n",
    "    # Initialize conversation history\n",
    "    history = []\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # TODO: Get response using conversation history\n",
    "        response = get_response(user_input, api_key, history=history)\n",
    "        if response:\n",
    "            print(f\"Model: {response}\")\n",
    "            history.append(\n",
    "                {\"role\": \"assistant\", \"content\": response}\n",
    "            )\n",
    "            print(history)\n",
    "        else:\n",
    "            print(\"Model: Sorry, I couldn't process that.\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Chat with an LLM using conversation history\")\n",
    "    # TODO: Add arguments to the parser\n",
    "    parser.add_argument('--prompt', type=str,default= '', help='Initial prompt to send to the model')\n",
    "    parser.add_argument('--key', type=str, required = True ,help='Hugging Face API key (required)')\n",
    "    args = parser.parse_args()\n",
    "    key = args.key\n",
    "    prompt = args.prompt\n",
    "\n",
    "    # TODO: Run the chat function with parsed arguments\n",
    "    if prompt == '':\n",
    "        run_chat(api_key=key)\n",
    "    else:\n",
    "        response = get_response(prompt, key)\n",
    "        if response:\n",
    "            print(f\"Model: {response}\")\n",
    "        else:\n",
    "            print(\"Model: Sorry, I couldn't process that.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing and Evaluation\n",
    "\n",
    "Create a script to test your chat implementations with specific healthcare questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils/test_chat.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/test_chat.py\n",
    "# utils/test_chat.py\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our chat modules - since we're in the same directory\n",
    "from one_off_chat import get_response as get_one_off_response\n",
    "# Optionally import the conversation module if testing that too\n",
    "# from conversation import get_response as get_contextual_response\n",
    "\n",
    "def test_chat(questions, model_name=\"google/flan-t5-base\", api_key=None):\n",
    "    \"\"\"\n",
    "    Test the chat function with a list of questions\n",
    "    \n",
    "    Args:\n",
    "        questions: A list of questions to test\n",
    "        model_name: Name of the model to use\n",
    "        api_key: API key for authentication\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary mapping questions to responses\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"Testing question: {question}\")\n",
    "        # Get response using the one-off chat function\n",
    "        response = get_one_off_response(question, model_name, api_key)\n",
    "        results[question] = response\n",
    "        \n",
    "    return results\n",
    "\n",
    "def save_results(results, output_file=\"results/part_2/example.txt\"):\n",
    "    \"\"\"\n",
    "    Save the test results to a file\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary mapping questions to responses\n",
    "        output_file: Path to the output file\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        # Write header\n",
    "        f.write(\"# LLM Chat Tool Test Results\\n\\n\")\n",
    "        \n",
    "        # Write usage examples\n",
    "        f.write(\"## Usage Examples\\n\\n\")\n",
    "        f.write(\"```bash\\n\")\n",
    "        f.write(\"# Run the one-off chat\\n\")\n",
    "        f.write(\"python utils/one_off_chat.py\\n\\n\")\n",
    "        f.write(\"# Run the contextual chat\\n\")\n",
    "        f.write(\"python utils/conversation.py\\n\")\n",
    "        f.write(\"```\\n\\n\")\n",
    "        \n",
    "        # Write test results\n",
    "        f.write(\"## Test Results\\n\\n\")\n",
    "        f.write(\"```csv\\n\")\n",
    "        f.write(\"question,response\\n\")\n",
    "        \n",
    "        for question, response in results.items():\n",
    "            # Format the question and response for CSV\n",
    "            q = question.replace(',', '').replace('\\n', ' ')\n",
    "            r = response.replace(',', '').replace('\\n', ' ')\n",
    "            f.write(f\"{q},{r}\\n\")\n",
    "            \n",
    "        f.write(\"```\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of healthcare questions to test\n",
    "    test_questions = [\n",
    "        \"What are the symptoms of gout?\",\n",
    "        \"How is gout diagnosed?\",\n",
    "        \"What treatments are available for gout?\",\n",
    "        \"What lifestyle changes can help manage gout?\",\n",
    "        \"What foods should be avoided with gout?\"\n",
    "    ]\n",
    "    \n",
    "    results = test_chat(test_questions)\n",
    "    save_results(results)\n",
    "    print(\"Test results saved to results/part_2/example.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test questions:\n",
      "1. What are the symptoms of gout?\n",
      "2. How is gout diagnosed?\n",
      "3. What treatments are available for gout?\n",
      "4. What lifestyle changes can help manage gout?\n",
      "5. What foods should be avoided with gout?\n"
     ]
    }
   ],
   "source": [
    "# List of healthcare questions to test\n",
    "test_questions = [\n",
    "    \"What are the symptoms of gout?\",\n",
    "    \"How is gout diagnosed?\",\n",
    "    \"What treatments are available for gout?\",\n",
    "    \"What lifestyle changes can help manage gout?\",\n",
    "    \"What foods should be avoided with gout?\"\n",
    "]\n",
    "\n",
    "# Display the test questions\n",
    "print(\"Test questions:\")\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"{i}. {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Checkpoints\n",
    "\n",
    "1. **API Connection**:\n",
    "   - [ ] Successfully connect to the Hugging Face API\n",
    "   - [ ] Send a query and receive a response\n",
    "   - [ ] Handle API errors gracefully\n",
    "\n",
    "2. **Chat Function Implementation**:\n",
    "   - [ ] Implement the get_response function\n",
    "   - [ ] Create the run_chat function for interactive sessions\n",
    "   - [ ] Handle errors and edge cases\n",
    "\n",
    "3. **Command Line Interface**:\n",
    "   - [ ] Create a parser with appropriate arguments\n",
    "   - [ ] Implement the main function\n",
    "   - [ ] Test the CLI functionality\n",
    "\n",
    "4. **Testing and Evaluation**:\n",
    "   - [ ] Test the functions with healthcare questions\n",
    "   - [ ] Save the results in a structured format\n",
    "   - [ ] Analyze the quality of responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Issues and Solutions\n",
    "\n",
    "1. **API Access Issues**:\n",
    "   - Problem: Rate limiting\n",
    "   - Solution: Implement exponential backoff and retry logic\n",
    "   - Problem: Authentication errors\n",
    "   - Solution: Verify API key and environment variables\n",
    "\n",
    "2. **Response Parsing Issues**:\n",
    "   - Problem: Unexpected response format\n",
    "   - Solution: Add error handling for different response structures\n",
    "   - Problem: Empty or error responses\n",
    "   - Solution: Provide meaningful fallback responses\n",
    "\n",
    "3. **CLI Issues**:\n",
    "   - Problem: Arguments not parsed correctly\n",
    "   - Solution: Test with different argument combinations\n",
    "   - Problem: Script not executable\n",
    "   - Solution: Check file permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Submit\n",
    "\n",
    "1. Your implementation of the chat scripts:\n",
    "   - Basic requirement: `utils/one_off_chat.py` for single prompt/response chat\n",
    "   - Stretch goal (optional): `utils/conversation.py` for contextual chat\n",
    "   - Testing script: `utils/test_chat.py` to evaluate your implementation\n",
    "\n",
    "2. Test results in `results/part_2/example.txt` with the following format:\n",
    "   - Usage examples section showing how to run your scripts\n",
    "   - Test results section with CSV-formatted question/response pairs\n",
    "   - If you implemented the stretch goal, include examples of contextual exchanges\n",
    "\n",
    "The auto-grader should check:\n",
    "1. That your chat scripts can be executed\n",
    "2. That they correctly handle the test questions\n",
    "3. That your results file contains the required sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test your implementation\n",
    "# Make sure to complete the TODO items in the scripts first!\n",
    "\n",
    "# Run the test script (uncomment when your implementation is ready)\n",
    "# %run utils/test_chat.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
